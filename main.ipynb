{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Projects/RLHF4Hate/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "class RoBERTaRewardModel(nn.Module):\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.classifier = torch.nn.Linear(768, 1)\n",
    "\n",
    "\n",
    "    def forward(self, good_input_ids, bad_input_ids, good_attention_mask, bad_attention_mask):\n",
    "\n",
    "        chosen, rejected = good_input_ids, bad_input_ids\n",
    "\n",
    "        chosen_reward = self.classifier(self.roberta(chosen, attention_mask=good_attention_mask))\n",
    "        rejected_reward = self.classifier(self.roberta(rejected, attention_mask=bad_attention_mask))\n",
    "\n",
    "        return chosen_reward, rejected_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RewardLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, chosen_reward, rejected_reward):\n",
    "        # Calculate the difference and apply sigmoid\n",
    "        return -torch.log(torch.sigmoid(chosen_reward - rejected_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of tuples): Each tuple contains a context, good example, and bad example string.\n",
    "            tokenizer (RobertaTokenizer): RoBERTa tokenizer for encoding inputs.\n",
    "            max_length (int): Maximum length of tokenized inputs.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Unpack the tuple containing context, good example, and bad example\n",
    "        context, good_example, bad_example = self.data[idx]\n",
    "\n",
    "        # Format as required: \"[CLS] Context [SEP] Good example [SEP]\" and \"[CLS] Context [SEP] Bad example [SEP]\"\n",
    "        good_input = f\"<s> {context} </s> {good_example} </s>\"\n",
    "        bad_input = f\"<s> {context} </s> {bad_example} </s>\"\n",
    "\n",
    "        # Tokenize each input with RoBERTa's tokenizer\n",
    "        good_encoding = self.tokenizer(\n",
    "            good_input, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        bad_encoding = self.tokenizer(\n",
    "            bad_input, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Extract input IDs and attention masks, and squeeze to remove extra dimensions\n",
    "        good_input_ids = good_encoding['input_ids'].squeeze()\n",
    "        good_attention_mask = good_encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        bad_input_ids = bad_encoding['input_ids'].squeeze()\n",
    "        bad_attention_mask = bad_encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"good_input_ids\": good_input_ids,\n",
    "            \"bad_input_ids\": bad_input_ids,\n",
    "            \"good_attention_mask\": good_attention_mask,\n",
    "            \"bad_attention_mask\": bad_attention_mask\n",
    "        }\n",
    "\n",
    "# Example data: List of tuples with (context, good example, bad example)\n",
    "data = [\n",
    "    (\"The sun is shining\", \"It is a bright day\", \"It is raining\"),\n",
    "    (\"The car is fast\", \"It accelerates quickly\", \"It moves slowly\"),\n",
    "    # Add more data points as needed\n",
    "]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SentencePairDataset(data, tokenizer, max_length=64)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModelTrainer:\n",
    "    def __init__(model, dataloader, epochs, lr):\n",
    "        self.optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = RewardLoss()\n",
    "        self.model = model\n",
    "        self.dataloader = dataset\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        best_loss = float('inf')  # Initialize best loss as infinity\n",
    "        checkpoint_path = \"best_model_checkpoint.pth\"  # Path to save the best model\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            for batch in self.dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through the model\n",
    "                chosen_reward, rejected_reward = self.model(**batch)\n",
    "\n",
    "                # Calculate the loss (you can use your custom loss function here)\n",
    "                loss = self.criterion(chosen_reward, rejected_reward)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimize the model\n",
    "                self.optimizer.step()\n",
    "\n",
    "                print(f\"Epoch {e+1}/{self.epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "                # Save the model checkpoint if we have a new best loss\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss = loss.item()\n",
    "                    print(f\"New best loss: {best_loss}. Saving model checkpoint...\")\n",
    "                    torch.save(self.model.state_dict(), checkpoint_path)\n",
    "\n",
    "        print(f\"Training finished. Best model saved to {checkpoint_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
